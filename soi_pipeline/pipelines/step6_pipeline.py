# pytracking/soi_pipeline/pipelines/step6_pipeline_enhanced.py
import os
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import random
from collections import defaultdict

from ..utils.verification_utils import load_jsonl, save_jsonl, compute_iou, xywh_to_xyxy

# åœ¨step6_pipeline.pyä¸­
from ..configs.config import HumanExperimentConfig, Config, BatchConfig


def filter_vlm_failed_samples(step5_dir: str, config: HumanExperimentConfig) -> List[Dict]:
    """
    ç­›é€‰VLMå¤±è´¥çš„æ ·æœ¬
    
    ç­›é€‰é€»è¾‘ï¼š
    1. IoU_1234 < é˜ˆå€¼
    2. è¯­ä¹‰å±‚çº§è¶Šä¸°å¯Œæ•ˆæœè¶Šå·®ï¼ˆlevel degradationï¼‰
    3. ç¡®ä¿æ ·æœ¬åˆ†å¸ƒåˆç†
    """
    print("ğŸ” Filtering VLM failed samples...")
    
    failed_samples = []
    
    # éå†æ‰€æœ‰éªŒè¯ç»“æœæ–‡ä»¶
    for filename in os.listdir(step5_dir):
        if not filename.endswith("_verify.jsonl"):
            continue
            
        seq_name = filename.replace("_verify.jsonl", "")
        file_path = os.path.join(step5_dir, filename)
        
        try:
            records = load_jsonl(file_path)
        except Exception as e:
            print(f"âŒ Failed to load {filename}: {e}")
            continue
        
        seq_failed_samples = []
        
        for record in records:
            # æ£€æŸ¥æ˜¯å¦ä¸ºVLMå¤±è´¥æ ·æœ¬
            is_failed = False
            failure_reasons = []
            
            # æ¡ä»¶1: IoUè¿‡ä½
            iou_1234 = record.get("iou_1234", 0)
            if iou_1234 < config.vlm_failure_threshold:
                is_failed = True
                failure_reasons.append(f"low_iou_{iou_1234:.3f}")
            
            # æ¡ä»¶2: è¯­ä¹‰å±‚çº§é€€åŒ–ï¼ˆè¶Šè¯¦ç»†æ•ˆæœè¶Šå·®ï¼‰
            level_ious = []
            for level in ["12", "123", "1234"]:
                iou = record.get(f"iou_{level}", 0)  # iou_
                level_ious.append(iou)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é€€åŒ–è¶‹åŠ¿
            if len(level_ious) >= 2:
                degradation_count = 0
                for i in range(1, len(level_ious)):
                    if level_ious[i] < level_ious[i-1]:
                        degradation_count += 1
                
                # å¦‚æœå¤§éƒ¨åˆ†å±‚çº§éƒ½åœ¨é€€åŒ–
                if degradation_count >= len(level_ious) - 1:
                    is_failed = True
                    failure_reasons.append("semantic_degradation")
            
            # æ¡ä»¶3: æ‰€æœ‰å±‚çº§éƒ½å¤±è´¥
            all_failed = all(not record.get(f"ok_{level}", False) 
                           for level in ["12", "123", "1234"])
            if all_failed:
                is_failed = True
                failure_reasons.append("all_levels_failed")
            
            if is_failed:
                record["failure_reasons"] = failure_reasons
                record["sequence_name"] = seq_name
                seq_failed_samples.append(record)
        
        # æ§åˆ¶æ¯ä¸ªåºåˆ—çš„æ ·æœ¬æ•°é‡
        if seq_failed_samples:
            # æŒ‰IoUæ’åºï¼Œä¼˜å…ˆé€‰æ‹©ä¸­ç­‰éš¾åº¦çš„æ ·æœ¬
            seq_failed_samples.sort(key=lambda x: x.get("iou_1234", 0))
            
            # é€‰æ‹©åˆé€‚æ•°é‡çš„æ ·æœ¬
            num_samples = min(max(config.min_samples_per_sequence, len(seq_failed_samples)), 
                            config.max_samples_per_sequence)
            
            # ä»ä¸­é—´éƒ¨åˆ†é€‰æ‹©ï¼ˆé¿å…è¿‡äºç®€å•æˆ–è¿‡äºå›°éš¾ï¼‰
            start_idx = max(0, (len(seq_failed_samples) - num_samples) // 2)
            selected_samples = seq_failed_samples[start_idx:start_idx + num_samples]
            
            failed_samples.extend(selected_samples)
            print(f"ğŸ“‹ Selected {len(selected_samples)} failed samples from {seq_name}")
    
    print(f"âœ… Total VLM failed samples: {len(failed_samples)}")
    return failed_samples

def analyze_failure_patterns(failed_samples: List[Dict]) -> Dict:
    """åˆ†æVLMå¤±è´¥æ¨¡å¼"""
    print("ğŸ“Š Analyzing VLM failure patterns...")
    
    analysis = {
        "total_samples": len(failed_samples),
        "failure_reasons": {},
        "iou_distribution": [],
        "sequence_distribution": {},
        "level_performance": {}
    }
    
    # ç»Ÿè®¡å¤±è´¥åŸå› 
    for sample in failed_samples:
        reasons = sample.get("failure_reasons", [])
        for reason in reasons:
            analysis["failure_reasons"][reason] = analysis["failure_reasons"].get(reason, 0) + 1
        
        # IoUåˆ†å¸ƒ
        analysis["iou_distribution"].append(sample.get("iou_1234", 0))
        
        # åºåˆ—åˆ†å¸ƒ
        seq_name = sample.get("sequence_name", "unknown")
        analysis["sequence_distribution"][seq_name] = analysis["sequence_distribution"].get(seq_name, 0) + 1
        
        # å„å±‚çº§æ€§èƒ½
        for level in ["12", "123", "1234"]:
            if level not in analysis["level_performance"]:
                analysis["level_performance"][level] = {"success": 0, "total": 0, "avg_iou": []}
            
            analysis["level_performance"][level]["total"] += 1
            if sample.get(f"ok_{level}", False):
                analysis["level_performance"][level]["success"] += 1
            
            iou = sample.get(f"iou_{level}", 0)
            analysis["level_performance"][level]["avg_iou"].append(iou)
    
    # è®¡ç®—å¹³å‡IoU
    for level in analysis["level_performance"]:
        ious = analysis["level_performance"][level]["avg_iou"]
        analysis["level_performance"][level]["avg_iou"] = np.mean(ious) if ious else 0
    
    return analysis

def create_balanced_batches(failed_samples: List[Dict], 
                          batch_size: int = 200,
                          stratify_by: str = "sequence_name",
                          min_batch_size: int = 50) -> List[List[Dict]]:
    """
    åˆ›å»ºå¹³è¡¡çš„æ•°æ®æ‰¹æ¬¡
    
    å‚æ•°:
    - failed_samples: å¤±è´¥æ ·æœ¬åˆ—è¡¨
    - batch_size: æ¯æ‰¹çš„ç›®æ ‡å¤§å°
    - stratify_by: åˆ†å±‚ä¾æ® ("sequence_name", "iou_range", "failure_type")
    - min_batch_size: æœ€å°æ‰¹æ¬¡å¤§å°
    
    è¿”å›:
    - æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡æ˜¯æ ·æœ¬åˆ—è¡¨
    """
    print(f"ğŸ“¦ Creating balanced batches (target size: {batch_size})...")
    
    # æŒ‰æŒ‡å®šç­–ç•¥åˆ†ç»„
    if stratify_by == "sequence_name":
        groups = defaultdict(list)
        for sample in failed_samples:
            seq_name = sample.get("sequence_name", "unknown")
            groups[seq_name].append(sample)
    
    elif stratify_by == "iou_range":
        groups = defaultdict(list)
        for sample in failed_samples:
            iou = sample.get("iou_1234", 0)
            if iou < 0.1:
                range_key = "very_low_0.0-0.1"
            elif iou < 0.2:
                range_key = "low_0.1-0.2"
            elif iou < 0.3:
                range_key = "medium_0.2-0.3"
            elif iou < 0.4:
                range_key = "high_0.3-0.4"
            else:
                range_key = "very_high_0.4+"
            groups[range_key].append(sample)
    
    elif stratify_by == "failure_type":
        groups = defaultdict(list)
        for sample in failed_samples:
            reasons = sample.get("failure_reasons", [])
            primary_reason = reasons[0] if reasons else "unknown"
            if primary_reason.startswith("low_iou"):
                group_key = "low_iou"
            else:
                group_key = primary_reason
            groups[group_key].append(sample)
    
    else:
        # é»˜è®¤ï¼šä¸åˆ†ç»„ï¼Œéšæœºåˆ†æ‰¹
        groups = {"all": failed_samples}
    
    print(f"ğŸ“Š Found {len(groups)} groups: {list(groups.keys())}")
    for group_name, samples in groups.items():
        print(f"   - {group_name}: {len(samples)} samples")
    
    # åˆ›å»ºå¹³è¡¡æ‰¹æ¬¡
    batches = []
    group_names = list(groups.keys())
    group_indices = {name: 0 for name in group_names}
    
    while True:
        current_batch = []
        batch_full = False
        
        # ä¸ºå½“å‰æ‰¹æ¬¡ä»æ¯ä¸ªç»„ä¸­å–æ ·æœ¬
        for group_name in group_names:
            group_samples = groups[group_name]
            start_idx = group_indices[group_name]
            
            if start_idx >= len(group_samples):
                continue  # è¯¥ç»„å·²ç”¨å®Œ
            
            # è®¡ç®—è¯¥ç»„åœ¨å½“å‰æ‰¹æ¬¡ä¸­åº”å çš„æ ·æœ¬æ•°
            remaining_groups = sum(1 for gn in group_names 
                                 if group_indices[gn] < len(groups[gn]))
            if remaining_groups == 0:
                break
                
            samples_per_group = max(1, batch_size // remaining_groups)
            end_idx = min(start_idx + samples_per_group, len(group_samples))
            
            group_batch_samples = group_samples[start_idx:end_idx]
            current_batch.extend(group_batch_samples)
            group_indices[group_name] = end_idx
            
            if len(current_batch) >= batch_size:
                batch_full = True
                break
        
        if not current_batch:
            break  # æ‰€æœ‰ç»„éƒ½ç”¨å®Œäº†
        
        # å¦‚æœæ‰¹æ¬¡å¤ªå°ä¸”ä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œå°è¯•åˆå¹¶
        if len(current_batch) < min_batch_size:
            remaining_samples = []
            for group_name in group_names:
                group_samples = groups[group_name]
                start_idx = group_indices[group_name]
                remaining_samples.extend(group_samples[start_idx:])
            
            if remaining_samples:
                current_batch.extend(remaining_samples[:batch_size - len(current_batch)])
        
        if current_batch:
            # æ‰“ä¹±æ‰¹æ¬¡å†…çš„æ ·æœ¬é¡ºåº
            random.shuffle(current_batch)
            batches.append(current_batch)
            print(f"ğŸ“¦ Created batch {len(batches)}: {len(current_batch)} samples")
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»„éƒ½ç”¨å®Œ
        if all(group_indices[gn] >= len(groups[gn]) for gn in group_names):
            break
    
    print(f"âœ… Created {len(batches)} batches")
    return batches

# def prepare_human_experiment_data_batched(failed_samples: List[Dict], 
#                                         dataset, 
#                                         output_dir: str,
#                                         batch_config: Dict = None) -> Dict[str, str]:
#     """
#     å‡†å¤‡åˆ†æ‰¹çš„äººç±»å®éªŒæ•°æ®
    
#     å‚æ•°:
#     - failed_samples: å¤±è´¥æ ·æœ¬åˆ—è¡¨
#     - dataset: æ•°æ®é›†å¯¹è±¡
#     - output_dir: è¾“å‡ºç›®å½•
#     - batch_config: æ‰¹æ¬¡é…ç½®
#     """
#     print("ğŸ“ Preparing batched human experiment data...")
    
#     # é»˜è®¤æ‰¹æ¬¡é…ç½®
#     if batch_config is None:
#         batch_config = {
#             "batch_size": 200,  # æ¯æ‰¹200ä¸ªæ ·æœ¬
#             "stratify_by": "sequence_name",  # æŒ‰åºåˆ—åˆ†å±‚
#             "min_batch_size": 50,  # æœ€å°æ‰¹æ¬¡å¤§å°
#             "shuffle_samples": True,  # æ˜¯å¦æ‰“ä¹±æ ·æœ¬
#             "create_overlap": False,  # æ˜¯å¦åˆ›å»ºé‡å æ‰¹æ¬¡ç”¨äºéªŒè¯
#             "overlap_ratio": 0.1  # é‡å æ¯”ä¾‹
#         }
    
#     # åˆ›å»ºåºåˆ—æ˜ å°„
#     seq_map = {seq.name: seq for seq in dataset}
    
#     # å¦‚æœéœ€è¦æ‰“ä¹±æ ·æœ¬
#     if batch_config.shuffle_samples:  # ç›´æ¥è®¿é—®å±æ€§ï¼Œä¸ä½¿ç”¨get()æ–¹æ³•
#         random.shuffle(failed_samples)
    
#     # åˆ›å»ºæ‰¹æ¬¡
#     batches = create_balanced_batches(
#         failed_samples,
#         batch_size=batch_config.batch_size,  # ç›´æ¥è®¿é—®å±æ€§
#         stratify_by=batch_config.stratify_by,  # ç›´æ¥è®¿é—®å±æ€§
#         min_batch_size=batch_config.min_batch_size  # ç›´æ¥è®¿é—®å±æ€§
#     )
    
#     # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
#     batches_dir = os.path.join(output_dir, "experiment_batches")
#     os.makedirs(batches_dir, exist_ok=True)
    
#     batch_files = []
#     batch_metadata = []
    
#     for batch_idx, batch_samples in enumerate(batches):
#         print(f"ğŸ“ Processing batch {batch_idx + 1}/{len(batches)}...")
        
#         experiment_data = []
#         batch_stats = {
#             "batch_id": batch_idx + 1,
#             "total_samples": len(batch_samples),
#             "sequences": set(),
#             "iou_range": {"min": 1.0, "max": 0.0, "avg": 0.0},
#             "failure_types": defaultdict(int)
#         }
        
#         for i, sample in enumerate(batch_samples):
#             seq_name = sample.get("sequence_name")
#             frame_idx = sample.get("frame_idx")
            
#             if seq_name not in seq_map:
#                 continue
            
#             seq = seq_map[seq_name]
#             batch_stats["sequences"].add(seq_name)
            
#             # ç»Ÿè®¡ä¿¡æ¯
#             iou_1234 = sample.get("iou_1234", 0)
#             batch_stats["iou_range"]["min"] = min(batch_stats["iou_range"]["min"], iou_1234)
#             batch_stats["iou_range"]["max"] = max(batch_stats["iou_range"]["max"], iou_1234)
            
#             for reason in sample.get("failure_reasons", []):
#                 batch_stats["failure_types"][reason] += 1
            
#             # æ‰¾åˆ°å‚è€ƒå¸§ï¼ˆæ¨¡æ¿å›¾ï¼‰
#             template_frame_idx = 0  # é»˜è®¤ä½¿ç”¨é¦–å¸§
#             template_box = seq.ground_truth_rect[template_frame_idx].tolist()
            
#             # å‡†å¤‡å®éªŒæ¡ç›®
#             experiment_item = {
#                 "experiment_id": f"{seq_name}_{frame_idx:06d}",
#                 "batch_id": batch_idx + 1,
#                 "sample_id_in_batch": i + 1,
#                 "sequence_name": seq_name,
#                 "frame_idx": frame_idx,
#                 "template_frame_idx": template_frame_idx,
#                 "template_image_path": seq.frames[template_frame_idx],
#                 "current_image_path": seq.frames[frame_idx],
#                 "template_box": template_box,  # xywh format
#                 "ground_truth_box": sample.get("gt_box"),  # xywh format
                
#                 # VLMç”Ÿæˆçš„æè¿°
#                 "description_level_1": sample.get("vlm_output_cleaned", {}).get("level1", ""),
#                 "description_level_2": sample.get("vlm_output_cleaned", {}).get("level2", ""),
#                 "description_level_3": sample.get("vlm_output_cleaned", {}).get("level3", ""),
#                 "description_level_4": sample.get("vlm_output_cleaned", {}).get("level4", ""),
                
#                 # VLMéªŒè¯ç»“æœï¼ˆç”¨äºå¯¹æ¯”ï¼‰
#                 "vlm_results": {
#                     "iou_12": sample.get("iou_12", 0),
#                     "iou_123": sample.get("iou_123", 0), 
#                     "iou_1234": sample.get("iou_1234", 0),
#                     "success_12": sample.get("ok_12", False),
#                     "success_123": sample.get("ok_123", False),
#                     "success_1234": sample.get("ok_1234", False)
#                 },
                
#                 # å¤±è´¥ä¿¡æ¯
#                 "failure_reasons": sample.get("failure_reasons", []),
                
#                 # å¾…å¡«å…¥çš„äººç±»å®éªŒç»“æœ
#                 "human_results": {
#                     "selected_level": None,  # 1, 2, 3, 4, 5 (æ— æ³•ç¡®å®š)
#                     "bounding_box": None,    # [x, y, w, h]
#                     "confidence": None,      # 1-5
#                     "difficulty": None,      # 1-5
#                     "comments": "",
#                     "time_spent": 0         # ç§’
#                 },
                
#                 "status": "pending"  # pending, completed, skipped
#             }
            
#             experiment_data.append(experiment_item)
        
#         # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
#         batch_stats["sequences"] = list(batch_stats["sequences"])
#         if batch_samples:
#             ious = [s.get("iou_1234", 0) for s in batch_samples]
#             batch_stats["iou_range"]["avg"] = np.mean(ious)
        
#         # ä¿å­˜æ‰¹æ¬¡æ•°æ®
#         batch_filename = f"batch_{batch_idx + 1:03d}_experiment_data.jsonl"
#         batch_file_path = os.path.join(batches_dir, batch_filename)
#         save_jsonl(batch_file_path, experiment_data)
#         batch_files.append(batch_file_path)
        
#         # ä¿å­˜æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯
#         stats_filename = f"batch_{batch_idx + 1:03d}_stats.json"
#         stats_file_path = os.path.join(batches_dir, stats_filename)
#         with open(stats_file_path, 'w', encoding='utf-8') as f:
#             json.dump(batch_stats, f, ensure_ascii=False, indent=2, default=str)
        
#         batch_metadata.append({
#             "batch_id": batch_idx + 1,
#             "filename": batch_filename,
#             "stats_file": stats_filename,
#             "sample_count": len(experiment_data),
#             "sequences": batch_stats["sequences"],
#             "estimated_time_minutes": len(experiment_data) * 0.5
#         })
        
#         print(f"   âœ… Batch {batch_idx + 1}: {len(experiment_data)} samples saved")
    
#     # åˆ›å»ºé‡å æ‰¹æ¬¡ï¼ˆç”¨äºéªŒè¯æ ‡æ³¨ä¸€è‡´æ€§ï¼‰
#     if batch_config.create_overlap:
#         overlap_samples = create_overlap_batches(
#             batches, 
#             output_dir, 
#             overlap_ratio=batch_config.overlap_ratio
#         )
    
#     # ä¿å­˜æ‰¹æ¬¡æ€»è§ˆ
#     batch_summary = {
#         "total_batches": len(batches),
#         "total_samples": len(failed_samples),
#         "batch_config": batch_config,
#         "created_date": pd.Timestamp.now().isoformat(),
#         "batches": batch_metadata
#     }
    
#     summary_file = os.path.join(batches_dir, "batches_summary.json")
    
#     with open(summary_file, 'w', encoding='utf-8') as f:
#         # åœ¨ä¿å­˜ä¹‹å‰è½¬æ¢ BatchConfig å¯¹è±¡ä¸ºå­—å…¸
#         if 'batch_config' in batch_summary and isinstance(batch_summary['batch_config'], BatchConfig):
#             batch_summary['batch_config'] = batch_summary['batch_config'].to_dict()

#         # ç„¶åä¿å­˜
#         json.dump(batch_summary, f, ensure_ascii=False, indent=2)

    
#     # ç”Ÿæˆæ‰¹æ¬¡åˆ†é…å»ºè®®
#     generate_batch_assignment_guide(batches_dir, batch_metadata)
    
#     print(f"âœ… Created {len(batches)} experiment batches")
#     print(f"ğŸ“ Batches saved to: {batches_dir}")
    
#     return {
#         "batches_dir": batches_dir,
#         "batch_files": batch_files,
#         "summary_file": summary_file,
#         "total_batches": len(batches),
#         "total_samples": len(failed_samples)
#     }

def prepare_human_experiment_data_batched(failed_samples: List[Dict], 
                                        dataset, 
                                        output_dir: str,
                                        batch_config: BatchConfig = None) -> Dict[str, str]:
    """
    å‡†å¤‡åˆ†æ‰¹çš„äººç±»å®éªŒæ•°æ®
    
    å‚æ•°:
    - failed_samples: å¤±è´¥æ ·æœ¬åˆ—è¡¨
    - dataset: æ•°æ®é›†å¯¹è±¡
    - output_dir: è¾“å‡ºç›®å½•
    - batch_config: æ‰¹æ¬¡é…ç½®
    """
    print("ğŸ“ Preparing batched human experiment data...")
    
    # é»˜è®¤æ‰¹æ¬¡é…ç½®
    if batch_config is None:
        batch_config = BatchConfig()
    
    # åˆ›å»ºåºåˆ—æ˜ å°„
    seq_map = {seq.name: seq for seq in dataset}
    
    # å¦‚æœéœ€è¦æ‰“ä¹±æ ·æœ¬
    if batch_config.shuffle_samples:
        random.shuffle(failed_samples)
    
    # åˆ›å»ºæ‰¹æ¬¡
    batches = create_balanced_batches(
        failed_samples,
        batch_size=batch_config.batch_size,
        stratify_by=batch_config.stratify_by,
        min_batch_size=batch_config.min_batch_size
    )
    
    # åˆ›å»ºæ‰¹æ¬¡ç›®å½•å’Œå›¾åƒç›®å½•
    batches_dir = os.path.join(output_dir, "experiment_batches")
    os.makedirs(batches_dir, exist_ok=True)
    
    # åˆ›å»ºå›¾åƒç›®å½•
    images_dir = os.path.join(output_dir, "experiment_images")
    os.makedirs(images_dir, exist_ok=True)
    
    batch_files = []
    batch_metadata = []
    
    # è·Ÿè¸ªå·²å¤åˆ¶çš„å›¾åƒï¼Œé¿å…é‡å¤å¤åˆ¶
    copied_images = set()
    
    for batch_idx, batch_samples in enumerate(batches):
        print(f"ğŸ“ Processing batch {batch_idx + 1}/{len(batches)}...")
        
        # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºå•ç‹¬çš„å›¾åƒç›®å½•
        batch_images_dir = os.path.join(images_dir, f"batch_{batch_idx + 1:03d}")
        os.makedirs(batch_images_dir, exist_ok=True)
        
        experiment_data = []
        batch_stats = {
            "batch_id": batch_idx + 1,
            "total_samples": len(batch_samples),
            "sequences": set(),
            "iou_range": {"min": 1.0, "max": 0.0, "avg": 0.0},
            "failure_types": defaultdict(int)
        }
        
        for i, sample in enumerate(batch_samples):
            seq_name = sample.get("sequence_name")
            frame_idx = sample.get("frame_idx")
            
            if seq_name not in seq_map:
                continue
            
            seq = seq_map[seq_name]
            batch_stats["sequences"].add(seq_name)
            
            # ç»Ÿè®¡ä¿¡æ¯
            iou_1234 = sample.get("iou_1234", 0)
            batch_stats["iou_range"]["min"] = min(batch_stats["iou_range"]["min"], iou_1234)
            batch_stats["iou_range"]["max"] = max(batch_stats["iou_range"]["max"], iou_1234)
            
            for reason in sample.get("failure_reasons", []):
                batch_stats["failure_types"][reason] += 1
            
            # è®¾ç½®æ¨¡æ¿å¸§ï¼šä½¿ç”¨ç¬¬ä¸€å¸§å’Œå‰ä¸€å¸§
            first_frame_idx = 0  # ç¬¬ä¸€å¸§
            prev_frame_idx = max(0, frame_idx - 1)  # å‰ä¸€å¸§ï¼Œç¡®ä¿ä¸ä¼šæ˜¯è´Ÿæ•°
            
            # å¦‚æœå½“å‰å¸§å°±æ˜¯ç¬¬ä¸€å¸§ï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€å¸§å’Œç¬¬äºŒå¸§
            if frame_idx == 0:
                prev_frame_idx = min(1, len(seq.frames) - 1)
            
            # å‡†å¤‡å›¾åƒè·¯å¾„
            first_frame_path = seq.frames[first_frame_idx]
            prev_frame_path = seq.frames[prev_frame_idx]
            current_frame_path = seq.frames[frame_idx]
            
            # å¤åˆ¶å›¾åƒåˆ°æ‰¹æ¬¡ç›®å½•
            # åˆ›å»ºåºåˆ—å­ç›®å½•
            seq_dir = os.path.join(batch_images_dir, seq_name)
            os.makedirs(seq_dir, exist_ok=True)
            
            # ç›®æ ‡æ–‡ä»¶å
            first_frame_dest = os.path.join(seq_dir, f"frame_{first_frame_idx:06d}.jpg")
            prev_frame_dest = os.path.join(seq_dir, f"frame_{prev_frame_idx:06d}.jpg")
            current_frame_dest = os.path.join(seq_dir, f"frame_{frame_idx:06d}.jpg")
            
            # å¤åˆ¶å›¾åƒæ–‡ä»¶ï¼ˆå¦‚æœå°šæœªå¤åˆ¶ï¼‰
            image_paths = {
                "first_frame": (first_frame_path, first_frame_dest),
                "prev_frame": (prev_frame_path, prev_frame_dest),
                "current_frame": (current_frame_path, current_frame_dest)
            }
            
            for img_type, (src, dest) in image_paths.items():
                if dest not in copied_images:
                    try:
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(dest), exist_ok=True)
                        # å¤åˆ¶å›¾åƒ
                        import shutil
                        shutil.copy2(src, dest)
                        copied_images.add(dest)
                    except Exception as e:
                        print(f"âš ï¸ Failed to copy {img_type} image: {e}")
                        # å¦‚æœå¤åˆ¶å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„
                        if img_type == "first_frame":
                            first_frame_dest = first_frame_path
                        elif img_type == "prev_frame":
                            prev_frame_dest = prev_frame_path
                        elif img_type == "current_frame":
                            current_frame_dest = current_frame_path
            
            # å‡†å¤‡å®éªŒæ¡ç›®
            experiment_item = {
                "experiment_id": f"{seq_name}_{frame_idx:06d}",
                "batch_id": batch_idx + 1,
                "sample_id_in_batch": i + 1,
                "sequence_name": seq_name,
                "frame_idx": frame_idx,
                
                # æä¾›ä¸¤ä¸ªæ¨¡æ¿å¸§é€‰æ‹©
                "first_frame_idx": first_frame_idx,
                "prev_frame_idx": prev_frame_idx,
                "first_frame_image_path": first_frame_dest,
                "prev_frame_image_path": prev_frame_dest,
                "current_image_path": current_frame_dest,
                
                # ä¸¤ä¸ªæ¨¡æ¿å¸§çš„è¾¹ç•Œæ¡†
                "first_frame_box": seq.ground_truth_rect[first_frame_idx].tolist(),  # xywh format
                "prev_frame_box": seq.ground_truth_rect[prev_frame_idx].tolist(),  # xywh format
                
                "ground_truth_box": sample.get("gt_box"),  # xywh format
                
                # VLMç”Ÿæˆçš„æè¿°
                "description_level_1": sample.get("vlm_output_cleaned", {}).get("level1", ""),
                "description_level_2": sample.get("vlm_output_cleaned", {}).get("level2", ""),
                "description_level_3": sample.get("vlm_output_cleaned", {}).get("level3", ""),
                "description_level_4": sample.get("vlm_output_cleaned", {}).get("level4", ""),
                
                # VLMéªŒè¯ç»“æœï¼ˆç”¨äºå¯¹æ¯”ï¼‰
                "vlm_results": {
                    "iou_12": sample.get("iou_12", 0),
                    "iou_123": sample.get("iou_123", 0), 
                    "iou_1234": sample.get("iou_1234", 0),
                    "success_12": sample.get("ok_12", False),
                    "success_123": sample.get("ok_123", False),
                    "success_1234": sample.get("ok_1234", False)
                },
                
                # å¤±è´¥ä¿¡æ¯
                "failure_reasons": sample.get("failure_reasons", []),
                
                # å¾…å¡«å…¥çš„äººç±»å®éªŒç»“æœ
                "human_results": {
                    "selected_level": None,  # 1, 2, 3, 4, 5 (æ— æ³•ç¡®å®š)
                    "bounding_box": None,    # [x, y, w, h]
                    "confidence": None,      # 1-5
                    "difficulty": None,      # 1-5
                    "comments": "",
                    "time_spent": 0         # ç§’
                },
                
                "status": "pending"  # pending, completed, skipped
            }
            
            experiment_data.append(experiment_item)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        batch_stats["sequences"] = list(batch_stats["sequences"])
        if batch_samples:
            ious = [s.get("iou_1234", 0) for s in batch_samples]
            batch_stats["iou_range"]["avg"] = np.mean(ious)
        
        # ä¿å­˜æ‰¹æ¬¡æ•°æ®
        batch_filename = f"batch_{batch_idx + 1:03d}_experiment_data.jsonl"
        batch_file_path = os.path.join(batches_dir, batch_filename)
        save_jsonl(batch_file_path, experiment_data)
        batch_files.append(batch_file_path)
        
        # ä¿å­˜æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯
        stats_filename = f"batch_{batch_idx + 1:03d}_stats.json"
        stats_file_path = os.path.join(batches_dir, stats_filename)
        with open(stats_file_path, 'w', encoding='utf-8') as f:
            json.dump(batch_stats, f, ensure_ascii=False, indent=2, default=str)
        
        batch_metadata.append({
            "batch_id": batch_idx + 1,
            "filename": batch_filename,
            "images_dir": batch_images_dir,
            "stats_file": stats_filename,
            "sample_count": len(experiment_data),
            "sequences": batch_stats["sequences"],
            "estimated_time_minutes": len(experiment_data) * 0.5
        })
        
        print(f"   âœ… Batch {batch_idx + 1}: {len(experiment_data)} samples saved")
        print(f"   ğŸ“¸ Batch {batch_idx + 1}: Images saved to {batch_images_dir}")
    
    # åˆ›å»ºé‡å æ‰¹æ¬¡ï¼ˆç”¨äºéªŒè¯æ ‡æ³¨ä¸€è‡´æ€§ï¼‰
    if batch_config.create_overlap:
        overlap_samples = create_overlap_batches(
            batches, 
            output_dir, 
            overlap_ratio=batch_config.overlap_ratio
        )
    
    # ä¿å­˜æ‰¹æ¬¡æ€»è§ˆ
    batch_summary = {
        "total_batches": len(batches),
        "total_samples": len(failed_samples),
        "batch_config": batch_config.to_dict() if hasattr(batch_config, 'to_dict') else batch_config,
        "created_date": pd.Timestamp.now().isoformat(),
        "images_dir": images_dir,
        "batches": batch_metadata
    }
    
    summary_file = os.path.join(batches_dir, "batches_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(batch_summary, f, ensure_ascii=False, indent=2, default=str)
    
    # ç”Ÿæˆæ‰¹æ¬¡åˆ†é…å»ºè®®
    generate_batch_assignment_guide(batches_dir, batch_metadata)
    
    print(f"âœ… Created {len(batches)} experiment batches")
    print(f"ğŸ“ Batches saved to: {batches_dir}")
    print(f"ğŸ“¸ Images saved to: {images_dir}")
    
    return {
        "batches_dir": batches_dir,
        "images_dir": images_dir,
        "batch_files": batch_files,
        "summary_file": summary_file,
        "total_batches": len(batches),
        "total_samples": len(failed_samples)
    }

def create_overlap_batches(batches: List[List[Dict]], 
                          output_dir: str, 
                          overlap_ratio: float = 0.1) -> List[Dict]:
    """åˆ›å»ºé‡å æ‰¹æ¬¡ç”¨äºéªŒè¯æ ‡æ³¨ä¸€è‡´æ€§"""
    print(f"ğŸ”„ Creating overlap batches for consistency validation...")
    
    overlap_dir = os.path.join(output_dir, "overlap_validation")
    os.makedirs(overlap_dir, exist_ok=True)
    
    # ä»æ¯ä¸ªæ‰¹æ¬¡ä¸­éšæœºé€‰æ‹©æ ·æœ¬
    overlap_samples = []
    for batch_idx, batch in enumerate(batches):
        num_overlap = max(1, int(len(batch) * overlap_ratio))
        selected = random.sample(batch, min(num_overlap, len(batch)))
        
        for sample in selected:
            overlap_sample = sample.copy()
            overlap_sample["original_batch_id"] = batch_idx + 1
            overlap_sample["validation_type"] = "consistency_check"
            overlap_samples.append(overlap_sample)
    
    # éšæœºæ‰“ä¹±é‡å æ ·æœ¬
    random.shuffle(overlap_samples)
    
    # åˆ›å»ºé‡å éªŒè¯æ‰¹æ¬¡
    overlap_batch_size = 50  # é‡å æ‰¹æ¬¡è¾ƒå°
    overlap_batches = []
    
    for i in range(0, len(overlap_samples), overlap_batch_size):
        batch_samples = overlap_samples[i:i + overlap_batch_size]
        
        overlap_filename = f"overlap_batch_{len(overlap_batches) + 1:02d}.jsonl"
        overlap_file_path = os.path.join(overlap_dir, overlap_filename)
        save_jsonl(overlap_file_path, batch_samples)
        
        overlap_batches.append({
            "filename": overlap_filename,
            "sample_count": len(batch_samples),
            "source_batches": list(set(s["original_batch_id"] for s in batch_samples))
        })
    
    # ä¿å­˜é‡å æ‰¹æ¬¡ä¿¡æ¯
    overlap_summary = {
        "total_overlap_samples": len(overlap_samples),
        "overlap_ratio": overlap_ratio,
        "overlap_batches": overlap_batches
    }
    
    with open(os.path.join(overlap_dir, "overlap_summary.json"), 'w') as f:
        json.dump(overlap_summary, f, indent=2)
    
    print(f"âœ… Created {len(overlap_batches)} overlap validation batches")
    return overlap_samples

def generate_batch_assignment_guide(batches_dir: str, batch_metadata: List[Dict]):
    """ç”Ÿæˆæ‰¹æ¬¡åˆ†é…æŒ‡å—"""
    print("ğŸ“‹ Generating batch assignment guide...")
    
    guide_content = f"""# Human Experiment Batch Assignment Guide

## Overview
Total batches: {len(batch_metadata)}
Total samples: {sum(b['sample_count'] for b in batch_metadata)}

## Batch Distribution Strategy

### Recommended Assignment Plan

#### For Individual Annotators (1 person per batch):
"""
    
    for i, batch in enumerate(batch_metadata):
        estimated_hours = batch['estimated_time_minutes'] / 60
        guide_content += f"""
**Batch {batch['batch_id']}** ({batch['filename']})
- Samples: {batch['sample_count']}
- Estimated time: {batch['estimated_time_minutes']:.0f} minutes ({estimated_hours:.1f} hours)
- Sequences: {len(batch['sequences'])} unique sequences
- Recommended for: {'Experienced annotator' if batch['sample_count'] > 150 else 'Any annotator'}
"""

    guide_content += f"""

#### For Team Annotation (Multiple annotators):
- **Phase 1**: Distribute batches 1-{len(batch_metadata)//2} to Team A
- **Phase 2**: Distribute batches {len(batch_metadata)//2 + 1}-{len(batch_metadata)} to Team B
- **Cross-validation**: Use overlap batches for quality control

### Quality Control Recommendations

1. **Training Phase**: Start with 10-20 samples from Batch 1
2. **Consistency Check**: Use overlap validation batches
3. **Progress Monitoring**: Review completed batches regularly
4. **Break Schedule**: Recommend 15-minute breaks every hour

### Batch Characteristics

| Batch ID | Samples | Est. Time (hrs) | Difficulty | Priority |
|----------|---------|-----------------|------------|----------|"""

    for batch in batch_metadata:
        hours = batch['estimated_time_minutes'] / 60
        difficulty = "High" if batch['sample_count'] > 200 else "Medium" if batch['sample_count'] > 100 else "Low"
        priority = "High" if batch['batch_id'] <= 3 else "Medium"
        guide_content += f"""
| {batch['batch_id']} | {batch['sample_count']} | {hours:.1f} | {difficulty} | {priority} |"""

    guide_content += """

### Instructions for Batch Processing

1. **Before Starting**:
   - Read the experiment guide thoroughly
   - Complete the training phase
   - Set up a comfortable working environment

2. **During Annotation**:
   - Take regular breaks (every 30-45 minutes)
   - Save progress frequently (auto-save enabled)
   - Note any issues or unclear cases

3. **After Completion**:
   - Review your annotations for obvious errors
   - Submit the completed batch
   - Provide feedback on batch difficulty

### Contact Information
For questions or technical issues, contact: [Your Contact Information]
"""

    guide_file = os.path.join(batches_dir, "batch_assignment_guide.md")
    with open(guide_file, 'w', encoding='utf-8') as f:
        f.write(guide_content)
    
    print(f"âœ… Batch assignment guide saved to: {guide_file}")

def run_step6_preparation(config, step5_dir: str, dataset, 
                         experiment_config: Optional[HumanExperimentConfig] = None,
                         batch_config: Optional[Dict] = None) -> Dict[str, str]:
    """
    è¿è¡ŒStep 6å‡†å¤‡é˜¶æ®µï¼šVLMå¤±è´¥æ ·æœ¬ç­›é€‰å’Œåˆ†æ‰¹å®éªŒæ•°æ®å‡†å¤‡
    """
    print("ğŸ”„ Starting Step 6: Human-Machine Cognitive Difference Exploration")
    
    if experiment_config is None:
        experiment_config = HumanExperimentConfig()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(config.output_dir, "step6_human_experiment")
    os.makedirs(output_dir, exist_ok=True)
    
    results = {"output_dir": output_dir}
    
    # Step 6.1: ç­›é€‰VLMå¤±è´¥æ ·æœ¬
    print("\n" + "-"*50)
    print("Phase 1: VLM Failed Sample Filtering")
    print("-"*50)
    
    failed_samples = filter_vlm_failed_samples(step5_dir, experiment_config)
    
    # ä¿å­˜å¤±è´¥æ ·æœ¬
    failed_samples_file = os.path.join(output_dir, "vlm_failed_samples.jsonl")
    save_jsonl(failed_samples_file, failed_samples)
    results["failed_samples_file"] = failed_samples_file
    
    # Step 6.2: åˆ†æå¤±è´¥æ¨¡å¼
    print("\n" + "-"*50)
    print("Phase 2: Failure Pattern Analysis")
    print("-"*50)
    
    failure_analysis = analyze_failure_patterns(failed_samples)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_file = os.path.join(output_dir, "vlm_failure_analysis.json")
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(failure_analysis, f, ensure_ascii=False, indent=2)
    results["failure_analysis_file"] = analysis_file
    
    # ç”Ÿæˆåˆ†æå›¾è¡¨
    generate_failure_analysis_plots(failure_analysis, output_dir)
    
    # Step 6.3: å‡†å¤‡åˆ†æ‰¹äººç±»å®éªŒæ•°æ®
    print("\n" + "-"*50)
    print("Phase 3: Batched Human Experiment Data Preparation")
    print("-"*50)
    
    batch_results = prepare_human_experiment_data_batched(
        failed_samples, dataset, output_dir, batch_config
    )
    results.update(batch_results)
    
    # ç”Ÿæˆå®éªŒè¯´æ˜å’ŒIRBæ–‡æ¡£
    generate_experiment_documentation(output_dir, failure_analysis, len(failed_samples))
    
    print(f"\nâœ… Step 6 preparation completed!")
    print(f"ğŸ“ Results saved to: {output_dir}")
    print(f"ğŸ§ª Ready for human experiment with {len(failed_samples)} samples in {batch_results['total_batches']} batches")
    print(f"ğŸ“‹ Next steps:")
    print(f"   1. Review batch assignment guide: {batch_results['batches_dir']}/batch_assignment_guide.md")
    print(f"   2. Distribute batches to annotators")
    print(f"   3. Run Gradio annotation tool for each batch")
    print(f"   4. Collect and merge results")
    
    return results

# ä¿æŒåŸæœ‰çš„å…¶ä»–å‡½æ•°ä¸å˜...
def generate_failure_analysis_plots(analysis: Dict, output_dir: str, top_sequences: int = 20):
    """
    ç”Ÿæˆå¤±è´¥åˆ†æå›¾è¡¨
    
    å‚æ•°:
    - analysis: åˆ†æç»“æœå­—å…¸
    - output_dir: è¾“å‡ºç›®å½•
    - top_sequences: è¦æ˜¾ç¤ºçš„é¡¶éƒ¨åºåˆ—æ•°é‡ï¼Œé»˜è®¤ä¸º20
    """
    print("ğŸ“Š Generating failure analysis plots...")
    
    plots_dir = os.path.join(output_dir, "analysis_plots")
    os.makedirs(plots_dir, exist_ok=True)
    
    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # 1. å¤±è´¥åŸå› åˆ†å¸ƒé¥¼å›¾ - åˆå¹¶ç›¸ä¼¼åŸå› 
    if analysis["failure_reasons"]:
        plt.figure(figsize=(10, 8))
        
        # å¯¹å¤±è´¥åŸå› è¿›è¡Œåˆ†ç±»æ±‡æ€»
        categorized_reasons = {
            "Low IoU (0.0-0.1)": 0,
            "Low IoU (0.1-0.2)": 0,
            "Low IoU (0.2-0.3)": 0,
            "Low IoU (0.3-0.4)": 0,
            "Low IoU (0.4-0.5)": 0,
            "Semantic Degradation": 0,
            "All Levels Failed": 0,
            "Other": 0
        }
        
        for reason, count in analysis["failure_reasons"].items():
            if reason == "semantic_degradation":
                categorized_reasons["Semantic Degradation"] += count
            elif reason == "all_levels_failed":
                categorized_reasons["All Levels Failed"] += count
            elif reason.startswith("low_iou_"):
                try:
                    iou_value = float(reason.split("_")[-1])
                    if 0.0 <= iou_value < 0.1:
                        categorized_reasons["Low IoU (0.0-0.1)"] += count
                    elif 0.1 <= iou_value < 0.2:
                        categorized_reasons["Low IoU (0.1-0.2)"] += count
                    elif 0.2 <= iou_value < 0.3:
                        categorized_reasons["Low IoU (0.2-0.3)"] += count
                    elif 0.3 <= iou_value < 0.4:
                        categorized_reasons["Low IoU (0.3-0.4)"] += count
                    elif 0.4 <= iou_value < 0.5:
                        categorized_reasons["Low IoU (0.4-0.5)"] += count
                    else:
                        categorized_reasons["Other"] += count
                except:
                    categorized_reasons["Other"] += count
            else:
                categorized_reasons["Other"] += count
        
        # ç§»é™¤è®¡æ•°ä¸º0çš„ç±»åˆ«
        categorized_reasons = {k: v for k, v in categorized_reasons.items() if v > 0}
        
        reasons = list(categorized_reasons.keys())
        counts = list(categorized_reasons.values())
        
        plt.pie(counts, labels=reasons, autopct='%1.1f%%', startangle=90)
        plt.title('VLM Failure Reasons Distribution', fontsize=16, fontweight='bold')
        plt.axis('equal')
        plt.tight_layout()
        plt.savefig(os.path.join(plots_dir, "failure_reasons_pie.png"), dpi=300, bbox_inches='tight')
        plt.close()
    
    # 2. IoUåˆ†å¸ƒç›´æ–¹å›¾ - ä»failure_reasonsä¸­æå–IoUå€¼
    if analysis["failure_reasons"]:
        iou_values = []
        for reason, count in analysis["failure_reasons"].items():
            if reason.startswith("low_iou_"):
                try:
                    iou_value = float(reason.split("_")[-1])
                    # ä¸ºæ¯ä¸ªIoUå€¼æ·»åŠ å¯¹åº”æ•°é‡çš„æ ·æœ¬
                    iou_values.extend([iou_value] * count)
                except:
                    pass
        
        if iou_values:
            plt.figure(figsize=(12, 6))
            plt.hist(iou_values, bins=20, alpha=0.7, edgecolor='black')
            plt.xlabel('IoU Score', fontsize=12)
            plt.ylabel('Frequency', fontsize=12)
            plt.title('IoU Distribution of VLM Failed Samples', fontsize=16, fontweight='bold')
            plt.axvline(np.mean(iou_values), color='red', linestyle='--', 
                       label=f'Mean IoU: {np.mean(iou_values):.3f}')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(os.path.join(plots_dir, "iou_distribution.png"), dpi=300, bbox_inches='tight')
            plt.close()
    
    # 3. å„å±‚çº§æ€§èƒ½å¯¹æ¯”
    if analysis["level_performance"]:
        levels = list(analysis["level_performance"].keys())
        success_rates = []
        avg_ious = []
        
        for level in levels:
            perf = analysis["level_performance"][level]
            success_rate = perf["success"] / perf["total"] if perf["total"] > 0 else 0
            success_rates.append(success_rate)
            avg_ious.append(perf["avg_iou"])
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æˆåŠŸç‡
        bars1 = ax1.bar(levels, success_rates, alpha=0.8)
        ax1.set_ylabel('Success Rate', fontsize=12)
        ax1.set_xlabel('Semantic Level', fontsize=12)
        ax1.set_title('VLM Success Rate by Semantic Level', fontsize=14, fontweight='bold')
        ax1.set_ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars1, success_rates):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{rate:.2f}', ha='center', va='bottom')
        
        # å¹³å‡IoU
        bars2 = ax2.bar(levels, avg_ious, alpha=0.8, color='orange')
        ax2.set_ylabel('Average IoU', fontsize=12)
        ax2.set_xlabel('Semantic Level', fontsize=12)
        ax2.set_title('VLM Average IoU by Semantic Level', fontsize=14, fontweight='bold')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, iou in zip(bars2, avg_ious):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                    f'{iou:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(plots_dir, "level_performance_comparison.png"), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    # 4. åºåˆ—åˆ†å¸ƒæ¡å½¢å›¾ - æ˜¾ç¤ºæŒ‡å®šæ•°é‡çš„æœ€å¸¸è§åºåˆ—
    if analysis["sequence_distribution"]:
        plt.figure(figsize=(15, 8))
        sequences = list(analysis["sequence_distribution"].keys())
        counts = list(analysis["sequence_distribution"].values())
        
        # æŒ‰æ•°é‡æ’åº
        sorted_data = sorted(zip(sequences, counts), key=lambda x: x[1], reverse=True)
        # å–æŒ‡å®šæ•°é‡çš„æœ€å¸¸è§åºåˆ—
        top_n_sequences = sorted_data[:top_sequences]
        sequences, counts = zip(*top_n_sequences)
        
        plt.bar(range(len(sequences)), counts, alpha=0.8)
        plt.xlabel('Sequence', fontsize=12)
        plt.ylabel('Number of Failed Samples', fontsize=12)
        plt.title(f'Top {top_sequences} Sequences with Most VLM Failed Samples', fontsize=16, fontweight='bold')
        plt.xticks(range(len(sequences)), sequences, rotation=45, ha='right')
        plt.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()
        plt.savefig(os.path.join(plots_dir, "sequence_distribution.png"), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    print(f"âœ… Analysis plots saved to: {plots_dir}")

def generate_experiment_documentation(output_dir: str, analysis: Dict, num_samples: int):
    """ç”Ÿæˆå®éªŒæ–‡æ¡£å’ŒIRBç”³è¯·ææ–™"""
    print("ğŸ“‹ Generating experiment documentation...")
    
    docs_dir = os.path.join(output_dir, "experiment_docs")
    os.makedirs(docs_dir, exist_ok=True)
    
    # 1. å®éªŒè¯´æ˜æ–‡æ¡£
    experiment_guide = f"""# Human-Machine Cognitive Difference Experiment Guide

## Experiment Overview
This experiment aims to explore the cognitive differences between humans and Vision-Language Models (VLMs) in visual object tracking tasks. We have identified {num_samples} cases where VLMs failed to correctly locate target objects despite having semantic descriptions.

## Experiment Setup
- **Total Samples**: {num_samples}
- **Task**: Locate target objects in images using provided semantic descriptions
- **Interface**: Web-based annotation tool (Gradio)
- **Time Estimate**: ~{num_samples * 0.5:.0f} minutes ({num_samples * 0.5 / 60:.1f} hours)
- **Data Organization**: Divided into manageable batches for better quality control

## Instructions for Participants

### Task Description
You will be shown pairs of images:
1. **Template Image**: Shows the target object in its initial appearance
2. **Current Image**: Contains the target object you need to locate

### Semantic Information Levels
You can choose to use different levels of semantic information:
1. **Visual Only**: Use only visual information from the template image
2. **+ Position**: Add position/location description
3. **+ Appearance**: Add appearance/visual features description  
4. **+ Dynamics**: Add motion/behavior description
5. **+ Context**: Add environmental context and distractor information
6. **Cannot Determine**: If the target cannot be reliably located

### How to Annotate
1. **Select Information Level**: Choose which level of information helps you most
2. **Draw Bounding Box**: Click and drag to draw a box around the target object
3. **Rate Confidence**: How confident are you in your annotation? (1-5 scale)
4. **Rate Difficulty**: How difficult was this case? (1-5 scale)
5. **Add Comments**: Optional feedback about the task

### Quality Guidelines
- Draw tight bounding boxes around the target object
- Be honest about your confidence and the information level used
- Add comments if you notice issues with the descriptions
- Take your time - accuracy is more important than speed

### Batch Processing Guidelines
- Work on one batch at a time
- Take breaks between batches (recommended: 15 minutes every hour)
- Save progress frequently (auto-save enabled)
- Complete batches in the suggested order when possible

## Data Privacy and Ethics
- All data is anonymized and used for research purposes only
- No personal information is collected
- Participation is voluntary and can be stopped at any time
- Results will be used to improve computer vision systems

## Contact Information
For questions about this experiment, please contact: [Your Contact Information]
"""

    with open(os.path.join(docs_dir, "experiment_guide.md"), 'w', encoding='utf-8') as f:
        f.write(experiment_guide)
    
    # 2. IRBç”³è¯·æ‘˜è¦
    irb_summary = f"""# IRB Application Summary: Human-Machine Cognitive Difference Study

## Study Title
Exploring Semantic Cognition Differences Between Humans and Vision-Language Models in Visual Object Tracking

## Principal Investigator
[Your Name and Affiliation]

## Study Overview

### Background and Rationale
Vision-Language Models (VLMs) represent a significant advancement in AI, combining visual and textual understanding. However, their cognitive processes may differ significantly from human perception. This study investigates these differences in the context of visual object tracking, where both humans and VLMs must locate objects using semantic descriptions.

Our preliminary analysis of VLM performance identified {num_samples} challenging cases where state-of-the-art models failed to correctly locate target objects despite having detailed semantic descriptions. This presents an opportunity to understand:
1. How humans utilize different levels of semantic information
2. Where human cognition excels compared to current AI systems
3. How to improve AI systems based on human cognitive strategies

### Research Questions
1. **Primary**: Do humans perform better than VLMs when given the same semantic descriptions for visual object tracking?
2. **Secondary**: What levels of semantic information (position, appearance, dynamics, context) are most useful for humans vs. VLMs?
3. **Exploratory**: What characteristics make certain cases difficult for both humans and VLMs?

### Study Design

#### Participants
- **Target Sample Size**: 20-30 participants
- **Inclusion Criteria**: Adults (18+) with normal or corrected vision
- **Exclusion Criteria**: Vision impairments that affect object recognition
- **Recruitment**: Voluntary participation from [Your Institution/Platform]

#### Methodology
- **Design**: Comparative human-AI performance study
- **Task**: Web-based visual object localization using semantic descriptions
- **Data Collection**: Bounding box annotations, confidence ratings, information level preferences
- **Duration**: Approximately {num_samples * 0.5 / 60:.1f} hours per participant (divided into manageable batches)

#### Data Collection Procedure
1. **Consent**: Electronic informed consent before participation
2. **Training**: Brief tutorial on the annotation interface
3. **Main Task**: Annotate samples in batches using provided semantic descriptions
4. **Post-task**: Optional feedback survey

### Data and Privacy Protection

#### Data Collected
- **Visual Annotations**: Bounding box coordinates
- **Behavioral Data**: Information level preferences, confidence ratings, response times
- **Qualitative Data**: Optional comments and feedback
- **No Personal Data**: No names, emails, or identifying information collected

#### Data Security
- Data stored on secure servers with encryption
- Access limited to research team members
- Data retention: 5 years for research reproducibility
- Data sharing: Only aggregated, anonymous results will be published

#### Privacy Measures
- Participant assignment of anonymous IDs
- No linking between personal identity and responses
- Option to withdraw data at any time during participation

### Risk Assessment

#### Potential Risks
- **Minimal Risk Study**: No physical, psychological, or social risks beyond daily life
- **Eye Strain**: Possible minor eye fatigue from screen viewing
- **Time Commitment**: Approximately {num_samples * 0.5 / 60:.1f} hours of voluntary participation (divided into batches)

#### Risk Mitigation
- Clear instructions about taking breaks
- Voluntary participation with right to withdraw
- Task designed to minimize cognitive load
- Batch-based organization to prevent fatigue

### Benefits and Significance

#### Scientific Benefits
- Advance understanding of human vs. AI visual cognition
- Inform development of more human-like AI systems
- Contribute to computer vision and cognitive science literature

#### Societal Benefits
- Improve AI systems for applications like autonomous driving, medical imaging
- Better human-AI collaboration in visual tasks
- Enhanced AI safety through understanding failure modes

### Statistical Analysis Plan
- **Primary Analysis**: Compare human vs. VLM localization accuracy (IoU scores)
- **Secondary Analysis**: Analyze information level usage patterns
- **Exploratory Analysis**: Identify factors associated with task difficulty

### Ethical Considerations
- **Voluntary Participation**: Clear that participation is optional
- **Informed Consent**: Detailed explanation of study procedures
- **Data Minimization**: Collect only necessary data for research objectives
- **Transparency**: Results will be made publicly available

### Timeline
- **IRB Approval**: [Date]
- **Pilot Testing**: [Date range]
- **Data Collection**: [Date range] 
- **Analysis**: [Date range]
- **Publication**: [Date range]

## Statistical Power Analysis
Based on preliminary VLM results showing {analysis.get('level_performance', {}).get('1234', {}).get('avg_iou', 0):.3f} average IoU, we expect to detect meaningful differences in human performance with the proposed sample size.

## Data Management Plan
All data will be stored according to institutional data management policies, with regular backups and version control. Analysis code and aggregated results will be made available for reproducibility.

## Conclusion
This study addresses important questions about human-AI cognitive differences using a well-controlled experimental design with minimal risk to participants. The results will contribute to both scientific understanding and practical AI system development.
"""

    with open(os.path.join(docs_dir, "irb_application_summary.md"), 'w', encoding='utf-8') as f:
        f.write(irb_summary)
    
    # 3. å®éªŒé…ç½®æ–‡ä»¶
    experiment_config = {
        "experiment_info": {
            "title": "Human-Machine Cognitive Difference Study",
            "version": "1.0",
            "created_date": pd.Timestamp.now().isoformat(),
            "total_samples": num_samples,
            "estimated_duration_minutes": num_samples * 0.5
        },
        "ui_config": {
            "show_confidence_rating": True,
            "show_difficulty_rating": True,
            "enable_comments": True,
            "auto_save_interval": 30,  # seconds
            "max_time_per_sample": 300  # seconds
        },
        "semantic_levels": {
            "1": "Visual Only (Template Image)",
            "2": "+ Position/Location Information", 
            "3": "+ Appearance/Visual Features",
            "4": "+ Motion/Behavior Dynamics",
            "5": "+ Environmental Context & Distractors",
            "6": "Cannot Determine Target"
        },
        "rating_scales": {
            "confidence": {
                "1": "Very Low Confidence",
                "2": "Low Confidence", 
                "3": "Medium Confidence",
                "4": "High Confidence",
                "5": "Very High Confidence"
            },
            "difficulty": {
                "1": "Very Easy",
                "2": "Easy",
                "3": "Medium",
                "4": "Difficult", 
                "5": "Very Difficult"
            }
        }
    }
    
    with open(os.path.join(docs_dir, "experiment_config.json"), 'w', encoding='utf-8') as f:
        json.dump(experiment_config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Documentation saved to: {docs_dir}")
    print("ğŸ“‹ Generated files:")
    print("   - experiment_guide.md: Participant instructions")
    print("   - irb_application_summary.md: IRB application material")
    print("   - experiment_config.json: Technical configuration")