{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def create_interface(self) -> gr.Blocks:\n",
    "    #     \"\"\"ÂàõÂª∫GradioÁïåÈù¢\"\"\"\n",
    "    #     with gr.Blocks(title=\"Human-Machine Cognition Study\", theme=gr.themes.Soft()) as interface:\n",
    "    #         current_sample_state = gr.State(value=None)\n",
    "    #         annotation_start_time = gr.State(value=time.time())\n",
    "\n",
    "    #         gr.Markdown(\"\"\"\n",
    "    #         # üß† Human-Machine Cognitive Difference Study\n",
    "\n",
    "    #         Help us understand how humans and AI systems differ in visual object tracking!\n",
    "\n",
    "    #         ## üìã Instructions:\n",
    "    #         1. **Template Image** (left): Shows the target object with a red box\n",
    "    #         2. **Current Image** (right): Find the same object in this image\n",
    "    #         3. **Select Information Level**: Choose what information helps you most\n",
    "    #         4. **Draw Bounding Box**: Click and drag to mark the target object\n",
    "    #         5. **Rate & Submit**: Provide confidence and difficulty ratings\n",
    "    #         \"\"\")\n",
    "\n",
    "    #         progress_display = gr.Markdown(value=self.create_progress_info())\n",
    "    #         with gr.Row():\n",
    "    #             with gr.Column(scale=1):\n",
    "    #                 gr.Markdown(\"### üéØ Template Image (Target Object)\")\n",
    "    #                 template_image = gr.Image(\n",
    "    #                     label=\"Template with Target Box\",\n",
    "    #                     type=\"numpy\",  # ÂøÖÈ°ª‰∏∫numpy\n",
    "    #                     interactive=False\n",
    "    #                 )\n",
    "    #             with gr.Column(scale=1):\n",
    "    #                 gr.Markdown(\"### üîç Current Image (Find the Target)\")\n",
    "    #                 current_image = gr.Image(\n",
    "    #                     label=\"Click and drag to draw bounding box\",\n",
    "    #                     tool=\"bbox\",    # ÂøÖÈ°ªÊåáÂÆö\n",
    "    #                     type=\"numpy\",   # ÂøÖÈ°ª‰∏∫numpy\n",
    "    #                     interactive=True\n",
    "    #                 )\n",
    "\n",
    "    #         gr.Markdown(\"### üìù Semantic Descriptions\")\n",
    "    #         descriptions_display = gr.Markdown(value=\"\")\n",
    "\n",
    "    #         with gr.Row():\n",
    "    #             with gr.Column(scale=2):\n",
    "    #                 semantic_level = gr.Radio(\n",
    "    #                     choices=[\n",
    "    #                         (\"1 - Visual Only (Template Image)\", 1),\n",
    "    #                         (\"2 - + Position/Location Info\", 2),\n",
    "    #                         (\"3 - + Appearance/Visual Features\", 3),\n",
    "    #                         (\"4 - + Motion/Behavior Dynamics\", 4),\n",
    "    #                         (\"5 - + Environmental Context\", 5),\n",
    "    #                         (\"6 - Cannot Determine Target\", 6)\n",
    "    #                     ],\n",
    "    #                     label=\"üß† Which information level helped you most?\",\n",
    "    #                     value=1\n",
    "    #                 )\n",
    "    #             with gr.Column(scale=1):\n",
    "    #                 confidence_rating = gr.Slider(\n",
    "    #                     minimum=1, maximum=5, step=1, value=3,\n",
    "    #                     label=\"üéØ Confidence Level (1=Very Low, 5=Very High)\"\n",
    "    #                 )\n",
    "    #                 difficulty_rating = gr.Slider(\n",
    "    #                     minimum=1, maximum=5, step=1, value=3,\n",
    "    #                     label=\"‚ö° Task Difficulty (1=Very Easy, 5=Very Hard)\"\n",
    "    #                 )\n",
    "\n",
    "    #         comments_input = gr.Textbox(\n",
    "    #             label=\"üí≠ Comments (Optional)\",\n",
    "    #             placeholder=\"Any observations about this case?\",\n",
    "    #             lines=2\n",
    "    #         )\n",
    "\n",
    "    #         with gr.Row():\n",
    "    #             skip_button = gr.Button(\"‚è≠Ô∏è Skip This Sample\", variant=\"secondary\")\n",
    "    #             submit_button = gr.Button(\"‚úÖ Submit Annotation\", variant=\"primary\")\n",
    "    #             next_sample_button = gr.Button(\"‚û°Ô∏è Load Next Sample\", variant=\"secondary\")\n",
    "\n",
    "    #         status_display = gr.Markdown(value=\"\")\n",
    "\n",
    "    #         # ---- ‰∫§‰∫íÂáΩÊï∞ ----\n",
    "\n",
    "    #         def load_sample():\n",
    "    #             sample = self.get_current_sample()\n",
    "    #             if sample is None:\n",
    "    #                 return (\n",
    "    #                     None, None, \"üéâ **All samples completed!** Thank you for your participation!\",\n",
    "    #                     self.create_progress_info(), \"‚úÖ Experiment completed!\", sample, time.time()\n",
    "    #                 )\n",
    "    #             template_img = self.prepare_template_image(sample)\n",
    "    #             current_img = self.prepare_current_image(sample)\n",
    "    #             descriptions = self.format_descriptions(sample)\n",
    "    #             progress = self.create_progress_info()\n",
    "    #             status = f\"üìã **Sample {self.current_index + 1}**: {sample['sequence_name']} - Frame {sample['frame_idx']}\"\n",
    "    #             return (\n",
    "    #                 pil_to_numpy(template_img), pil_to_numpy(current_img), descriptions,\n",
    "    #                 progress, status, sample, time.time()\n",
    "    #             )\n",
    "\n",
    "    #         def submit_annotation(semantic_level, confidence, difficulty, comments,\n",
    "    #                               sample, start_time, current_img):\n",
    "    #             if sample is None:\n",
    "    #                 return \"‚ùå No sample to annotate\"\n",
    "    #             if not isinstance(current_img, dict) or 'boxes' not in current_img:\n",
    "    #                 return \"‚ùå Please draw a bounding box on the current image\"\n",
    "    #             bboxes = current_img['boxes']\n",
    "    #             if not bboxes or len(bboxes) == 0:\n",
    "    #                 return \"‚ùå Please draw a bounding box on the current image\"\n",
    "    #             x0, y0, x1, y1 = bboxes[0]\n",
    "    #             bbox = [float(x0), float(y0), float(x1 - x0), float(y1 - y0)]  # [x, y, w, h]\n",
    "    #             annotation_data = {\n",
    "    #                 \"selected_level\": semantic_level,\n",
    "    #                 \"bounding_box\": bbox,\n",
    "    #                 \"confidence\": confidence,\n",
    "    #                 \"difficulty\": difficulty,\n",
    "    #                 \"comments\": comments,\n",
    "    #                 \"time_spent\": time.time() - start_time\n",
    "    #             }\n",
    "    #             self.save_annotation(sample, annotation_data)\n",
    "    #             self.completed_count += 1\n",
    "    #             self.current_index += 1\n",
    "    #             return f\"‚úÖ Annotation saved! Sample {sample['experiment_id']} completed.\"\n",
    "\n",
    "    #         def skip_sample(sample):\n",
    "    #             if sample is None:\n",
    "    #                 return \"‚ùå No sample to skip\"\n",
    "    #             skip_data = {\n",
    "    #                 \"selected_level\": None,\n",
    "    #                 \"bounding_box\": None,\n",
    "    #                 \"confidence\": None,\n",
    "    #                 \"difficulty\": None,\n",
    "    #                 \"comments\": \"SKIPPED\",\n",
    "    #                 \"time_spent\": 0\n",
    "    #             }\n",
    "    #             result = {\n",
    "    #                 **sample,\n",
    "    #                 \"human_results\": skip_data,\n",
    "    #                 \"annotation_timestamp\": datetime.now().isoformat(),\n",
    "    #                 \"status\": \"skipped\"\n",
    "    #             }\n",
    "    #             with open(self.results_file, 'a', encoding='utf-8') as f:\n",
    "    #                 f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "    #             self.current_index += 1\n",
    "    #             return f\"‚è≠Ô∏è Sample {sample['experiment_id']} skipped.\"\n",
    "\n",
    "    #         # ‰∫ã‰ª∂ÁªëÂÆö\n",
    "    #         interface.load(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "    #         submit_button.click(\n",
    "    #             fn=submit_annotation,\n",
    "    #             inputs=[semantic_level, confidence_rating, difficulty_rating, comments_input,\n",
    "    #                     current_sample_state, annotation_start_time, current_image],\n",
    "    #             outputs=[status_display]\n",
    "    #         ).then(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "    #         skip_button.click(\n",
    "    #             fn=skip_sample,\n",
    "    #             inputs=[current_sample_state],\n",
    "    #             outputs=[status_display]\n",
    "    #         ).then(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "    #         next_sample_button.click(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "\n",
    "    #     return interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "# Ê∫êÁõÆÂΩïÔºö‰Ω†Â≠òÊîæ .jsonl Êñá‰ª∂ÁöÑË∑ØÂæÑ\n",
    "src_dir = \"/home/wyp/project/OSTrack/output/test/soi_online_tracking_results/ostrack/vitb_384_mae_ce_32x4_ep300/lasot/\"\n",
    "\n",
    "# ÁõÆÊ†áÁõÆÂΩïÔºö‰Ω†Â∏åÊúõÊää .jsonl Êñá‰ª∂Â§çÂà∂Âà∞ËøôÈáå\n",
    "dst_dir = \"/home/wyp/project/SUTrack/soi/trackers_status/ostrack_base_384\"\n",
    "\n",
    "# ÂàõÂª∫ÁõÆÊ†áÁõÆÂΩïÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# Êü•ÊâæÊâÄÊúâ .jsonl Êñá‰ª∂\n",
    "jsonl_files = glob(os.path.join(src_dir, \"*status.txt\"))\n",
    "\n",
    "# Â§çÂà∂ÊØè‰∏™Êñá‰ª∂\n",
    "for file_path in jsonl_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    dst_path = os.path.join(dst_dir, file_name)\n",
    "    shutil.copy2(file_path, dst_path)\n",
    "    print(f\"Copied {file_name} to {dst_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "# Ê∫êÁõÆÂΩïÔºö‰Ω†Â≠òÊîæ .jsonl Êñá‰ª∂ÁöÑË∑ØÂæÑ\n",
    "src_dir = \"/home/wyp/project/SUTrack/output/test/soi_online_tracking_results/sutrack/sutrack_l384/lasot\"\n",
    "\n",
    "# ÁõÆÊ†áÁõÆÂΩïÔºö‰Ω†Â∏åÊúõÊää .jsonl Êñá‰ª∂Â§çÂà∂Âà∞ËøôÈáå\n",
    "dst_dir = \"/home/wyp/project/SUTrack/soi/tracker_soi_results/sutrack_large_378\"\n",
    "\n",
    "# ÂàõÂª∫ÁõÆÊ†áÁõÆÂΩïÔºàÂ¶ÇÊûú‰∏çÂ≠òÂú®Ôºâ\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# Êü•ÊâæÊâÄÊúâ .jsonl Êñá‰ª∂\n",
    "jsonl_files = glob(os.path.join(src_dir, \"*.jsonl\"))\n",
    "\n",
    "# Â§çÂà∂ÊØè‰∏™Êñá‰ª∂\n",
    "for file_path in jsonl_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    dst_path = os.path.join(dst_dir, file_name)\n",
    "    shutil.copy2(file_path, dst_path)\n",
    "    print(f\"Copied {file_name} to {dst_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "config_file = 'rtmdet_tiny_8xb32-300e_coco.py'\n",
    "checkpoint_file = 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "inference_detector(model, 'demo/demo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']  # ÊåáÂÆö‰∏≠ÊñáÂ≠ó‰Ωì\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Ê≠£Â∏∏ÊòæÁ§∫Ë¥üÂè∑\n",
    "# Ê®°ÊãüÂèÇÊï∞\n",
    "np.random.seed(0)\n",
    "N = 100            # Ëø≠‰ª£Ê≠•Êï∞\n",
    "mu = 0.01          # Â≠¶‰π†Áéá\n",
    "true_w = 0.5       # ÁêÜÊÉ≥ÊùÉÈáç\n",
    "x = np.random.randn(N)          # ËæìÂÖ•‰ø°Âè∑\n",
    "d = true_w * x + 0.1*np.random.randn(N)  # Âä†Âô™ÁêÜÊÉ≥ËæìÂá∫\n",
    "\n",
    "# ÂàùÂßãÂåñ\n",
    "w = 0.0\n",
    "mse_list = []\n",
    "\n",
    "# LMSÁÆóÊ≥ï‰∏ªÂæ™ÁéØ\n",
    "for n in range(N):\n",
    "    y = w * x[n]               # È¢ÑÊµãËæìÂá∫\n",
    "    e = d[n] - y               # ËØØÂ∑Æ\n",
    "    w = w + mu * e * x[n]      # ÊùÉÂÄºÊõ¥Êñ∞\n",
    "    mse_list.append(e**2)      # ÂΩìÂâçMSEÔºàÁû¨Êó∂Ôºâ\n",
    "\n",
    "# Âπ≥ÊªëÊòæÁ§∫\n",
    "mse_smooth = np.convolve(mse_list, np.ones(5)/5, mode='valid')\n",
    "\n",
    "# ÁªòÂõæ\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(mse_smooth, label='MSE (smoothed)')\n",
    "plt.title('LMSÁÆóÊ≥ïÊî∂ÊïõËøáÁ®ãÔºàÂùáÊñπËØØÂ∑ÆÔºâ')\n",
    "plt.xlabel('Ëø≠‰ª£Ê≠• n')\n",
    "plt.ylabel('Áû¨Êó∂ËØØÂ∑ÆÂπ≥Êñπ $e^2(n)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
