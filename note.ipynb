{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def create_interface(self) -> gr.Blocks:\n",
    "    #     \"\"\"åˆ›å»ºGradioç•Œé¢\"\"\"\n",
    "    #     with gr.Blocks(title=\"Human-Machine Cognition Study\", theme=gr.themes.Soft()) as interface:\n",
    "    #         current_sample_state = gr.State(value=None)\n",
    "    #         annotation_start_time = gr.State(value=time.time())\n",
    "\n",
    "    #         gr.Markdown(\"\"\"\n",
    "    #         # ğŸ§  Human-Machine Cognitive Difference Study\n",
    "\n",
    "    #         Help us understand how humans and AI systems differ in visual object tracking!\n",
    "\n",
    "    #         ## ğŸ“‹ Instructions:\n",
    "    #         1. **Template Image** (left): Shows the target object with a red box\n",
    "    #         2. **Current Image** (right): Find the same object in this image\n",
    "    #         3. **Select Information Level**: Choose what information helps you most\n",
    "    #         4. **Draw Bounding Box**: Click and drag to mark the target object\n",
    "    #         5. **Rate & Submit**: Provide confidence and difficulty ratings\n",
    "    #         \"\"\")\n",
    "\n",
    "    #         progress_display = gr.Markdown(value=self.create_progress_info())\n",
    "    #         with gr.Row():\n",
    "    #             with gr.Column(scale=1):\n",
    "    #                 gr.Markdown(\"### ğŸ¯ Template Image (Target Object)\")\n",
    "    #                 template_image = gr.Image(\n",
    "    #                     label=\"Template with Target Box\",\n",
    "    #                     type=\"numpy\",  # å¿…é¡»ä¸ºnumpy\n",
    "    #                     interactive=False\n",
    "    #                 )\n",
    "    #             with gr.Column(scale=1):\n",
    "    #                 gr.Markdown(\"### ğŸ” Current Image (Find the Target)\")\n",
    "    #                 current_image = gr.Image(\n",
    "    #                     label=\"Click and drag to draw bounding box\",\n",
    "    #                     tool=\"bbox\",    # å¿…é¡»æŒ‡å®š\n",
    "    #                     type=\"numpy\",   # å¿…é¡»ä¸ºnumpy\n",
    "    #                     interactive=True\n",
    "    #                 )\n",
    "\n",
    "    #         gr.Markdown(\"### ğŸ“ Semantic Descriptions\")\n",
    "    #         descriptions_display = gr.Markdown(value=\"\")\n",
    "\n",
    "    #         with gr.Row():\n",
    "    #             with gr.Column(scale=2):\n",
    "    #                 semantic_level = gr.Radio(\n",
    "    #                     choices=[\n",
    "    #                         (\"1 - Visual Only (Template Image)\", 1),\n",
    "    #                         (\"2 - + Position/Location Info\", 2),\n",
    "    #                         (\"3 - + Appearance/Visual Features\", 3),\n",
    "    #                         (\"4 - + Motion/Behavior Dynamics\", 4),\n",
    "    #                         (\"5 - + Environmental Context\", 5),\n",
    "    #                         (\"6 - Cannot Determine Target\", 6)\n",
    "    #                     ],\n",
    "    #                     label=\"ğŸ§  Which information level helped you most?\",\n",
    "    #                     value=1\n",
    "    #                 )\n",
    "    #             with gr.Column(scale=1):\n",
    "    #                 confidence_rating = gr.Slider(\n",
    "    #                     minimum=1, maximum=5, step=1, value=3,\n",
    "    #                     label=\"ğŸ¯ Confidence Level (1=Very Low, 5=Very High)\"\n",
    "    #                 )\n",
    "    #                 difficulty_rating = gr.Slider(\n",
    "    #                     minimum=1, maximum=5, step=1, value=3,\n",
    "    #                     label=\"âš¡ Task Difficulty (1=Very Easy, 5=Very Hard)\"\n",
    "    #                 )\n",
    "\n",
    "    #         comments_input = gr.Textbox(\n",
    "    #             label=\"ğŸ’­ Comments (Optional)\",\n",
    "    #             placeholder=\"Any observations about this case?\",\n",
    "    #             lines=2\n",
    "    #         )\n",
    "\n",
    "    #         with gr.Row():\n",
    "    #             skip_button = gr.Button(\"â­ï¸ Skip This Sample\", variant=\"secondary\")\n",
    "    #             submit_button = gr.Button(\"âœ… Submit Annotation\", variant=\"primary\")\n",
    "    #             next_sample_button = gr.Button(\"â¡ï¸ Load Next Sample\", variant=\"secondary\")\n",
    "\n",
    "    #         status_display = gr.Markdown(value=\"\")\n",
    "\n",
    "    #         # ---- äº¤äº’å‡½æ•° ----\n",
    "\n",
    "    #         def load_sample():\n",
    "    #             sample = self.get_current_sample()\n",
    "    #             if sample is None:\n",
    "    #                 return (\n",
    "    #                     None, None, \"ğŸ‰ **All samples completed!** Thank you for your participation!\",\n",
    "    #                     self.create_progress_info(), \"âœ… Experiment completed!\", sample, time.time()\n",
    "    #                 )\n",
    "    #             template_img = self.prepare_template_image(sample)\n",
    "    #             current_img = self.prepare_current_image(sample)\n",
    "    #             descriptions = self.format_descriptions(sample)\n",
    "    #             progress = self.create_progress_info()\n",
    "    #             status = f\"ğŸ“‹ **Sample {self.current_index + 1}**: {sample['sequence_name']} - Frame {sample['frame_idx']}\"\n",
    "    #             return (\n",
    "    #                 pil_to_numpy(template_img), pil_to_numpy(current_img), descriptions,\n",
    "    #                 progress, status, sample, time.time()\n",
    "    #             )\n",
    "\n",
    "    #         def submit_annotation(semantic_level, confidence, difficulty, comments,\n",
    "    #                               sample, start_time, current_img):\n",
    "    #             if sample is None:\n",
    "    #                 return \"âŒ No sample to annotate\"\n",
    "    #             if not isinstance(current_img, dict) or 'boxes' not in current_img:\n",
    "    #                 return \"âŒ Please draw a bounding box on the current image\"\n",
    "    #             bboxes = current_img['boxes']\n",
    "    #             if not bboxes or len(bboxes) == 0:\n",
    "    #                 return \"âŒ Please draw a bounding box on the current image\"\n",
    "    #             x0, y0, x1, y1 = bboxes[0]\n",
    "    #             bbox = [float(x0), float(y0), float(x1 - x0), float(y1 - y0)]  # [x, y, w, h]\n",
    "    #             annotation_data = {\n",
    "    #                 \"selected_level\": semantic_level,\n",
    "    #                 \"bounding_box\": bbox,\n",
    "    #                 \"confidence\": confidence,\n",
    "    #                 \"difficulty\": difficulty,\n",
    "    #                 \"comments\": comments,\n",
    "    #                 \"time_spent\": time.time() - start_time\n",
    "    #             }\n",
    "    #             self.save_annotation(sample, annotation_data)\n",
    "    #             self.completed_count += 1\n",
    "    #             self.current_index += 1\n",
    "    #             return f\"âœ… Annotation saved! Sample {sample['experiment_id']} completed.\"\n",
    "\n",
    "    #         def skip_sample(sample):\n",
    "    #             if sample is None:\n",
    "    #                 return \"âŒ No sample to skip\"\n",
    "    #             skip_data = {\n",
    "    #                 \"selected_level\": None,\n",
    "    #                 \"bounding_box\": None,\n",
    "    #                 \"confidence\": None,\n",
    "    #                 \"difficulty\": None,\n",
    "    #                 \"comments\": \"SKIPPED\",\n",
    "    #                 \"time_spent\": 0\n",
    "    #             }\n",
    "    #             result = {\n",
    "    #                 **sample,\n",
    "    #                 \"human_results\": skip_data,\n",
    "    #                 \"annotation_timestamp\": datetime.now().isoformat(),\n",
    "    #                 \"status\": \"skipped\"\n",
    "    #             }\n",
    "    #             with open(self.results_file, 'a', encoding='utf-8') as f:\n",
    "    #                 f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "    #             self.current_index += 1\n",
    "    #             return f\"â­ï¸ Sample {sample['experiment_id']} skipped.\"\n",
    "\n",
    "    #         # äº‹ä»¶ç»‘å®š\n",
    "    #         interface.load(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "    #         submit_button.click(\n",
    "    #             fn=submit_annotation,\n",
    "    #             inputs=[semantic_level, confidence_rating, difficulty_rating, comments_input,\n",
    "    #                     current_sample_state, annotation_start_time, current_image],\n",
    "    #             outputs=[status_display]\n",
    "    #         ).then(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "    #         skip_button.click(\n",
    "    #             fn=skip_sample,\n",
    "    #             inputs=[current_sample_state],\n",
    "    #             outputs=[status_display]\n",
    "    #         ).then(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "    #         next_sample_button.click(\n",
    "    #             fn=load_sample,\n",
    "    #             inputs=[],\n",
    "    #             outputs=[template_image, current_image, descriptions_display,\n",
    "    #                     progress_display, status_display, current_sample_state, annotation_start_time]\n",
    "    #         )\n",
    "\n",
    "    #     return interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "# æºç›®å½•ï¼šä½ å­˜æ”¾ .jsonl æ–‡ä»¶çš„è·¯å¾„\n",
    "src_dir = \"/home/wyp/project/OSTrack/output/test/soi_online_tracking_results/ostrack/vitb_384_mae_ce_32x4_ep300/lasot/\"\n",
    "\n",
    "# ç›®æ ‡ç›®å½•ï¼šä½ å¸Œæœ›æŠŠ .jsonl æ–‡ä»¶å¤åˆ¶åˆ°è¿™é‡Œ\n",
    "dst_dir = \"/home/wyp/project/SUTrack/soi/trackers_status/ostrack_base_384\"\n",
    "\n",
    "# åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# æŸ¥æ‰¾æ‰€æœ‰ .jsonl æ–‡ä»¶\n",
    "jsonl_files = glob(os.path.join(src_dir, \"*status.txt\"))\n",
    "\n",
    "# å¤åˆ¶æ¯ä¸ªæ–‡ä»¶\n",
    "for file_path in jsonl_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    dst_path = os.path.join(dst_dir, file_name)\n",
    "    shutil.copy2(file_path, dst_path)\n",
    "    print(f\"Copied {file_name} to {dst_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "# æºç›®å½•ï¼šä½ å­˜æ”¾ .jsonl æ–‡ä»¶çš„è·¯å¾„\n",
    "src_dir = \"/home/wyp/project/SUTrack/output/test/soi_online_tracking_results/sutrack/sutrack_l384/lasot\"\n",
    "\n",
    "# ç›®æ ‡ç›®å½•ï¼šä½ å¸Œæœ›æŠŠ .jsonl æ–‡ä»¶å¤åˆ¶åˆ°è¿™é‡Œ\n",
    "dst_dir = \"/home/wyp/project/SUTrack/soi/tracker_soi_results/sutrack_large_378\"\n",
    "\n",
    "# åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# æŸ¥æ‰¾æ‰€æœ‰ .jsonl æ–‡ä»¶\n",
    "jsonl_files = glob(os.path.join(src_dir, \"*.jsonl\"))\n",
    "\n",
    "# å¤åˆ¶æ¯ä¸ªæ–‡ä»¶\n",
    "for file_path in jsonl_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    dst_path = os.path.join(dst_dir, file_name)\n",
    "    shutil.copy2(file_path, dst_path)\n",
    "    print(f\"Copied {file_name} to {dst_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "config_file = 'rtmdet_tiny_8xb32-300e_coco.py'\n",
    "checkpoint_file = 'rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "inference_detector(model, 'demo/demo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']  # æŒ‡å®šä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·\n",
    "# æ¨¡æ‹Ÿå‚æ•°\n",
    "np.random.seed(0)\n",
    "N = 100            # è¿­ä»£æ­¥æ•°\n",
    "mu = 0.01          # å­¦ä¹ ç‡\n",
    "true_w = 0.5       # ç†æƒ³æƒé‡\n",
    "x = np.random.randn(N)          # è¾“å…¥ä¿¡å·\n",
    "d = true_w * x + 0.1*np.random.randn(N)  # åŠ å™ªç†æƒ³è¾“å‡º\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "w = 0.0\n",
    "mse_list = []\n",
    "\n",
    "# LMSç®—æ³•ä¸»å¾ªç¯\n",
    "for n in range(N):\n",
    "    y = w * x[n]               # é¢„æµ‹è¾“å‡º\n",
    "    e = d[n] - y               # è¯¯å·®\n",
    "    w = w + mu * e * x[n]      # æƒå€¼æ›´æ–°\n",
    "    mse_list.append(e**2)      # å½“å‰MSEï¼ˆç¬æ—¶ï¼‰\n",
    "\n",
    "# å¹³æ»‘æ˜¾ç¤º\n",
    "mse_smooth = np.convolve(mse_list, np.ones(5)/5, mode='valid')\n",
    "\n",
    "# ç»˜å›¾\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(mse_smooth, label='MSE (smoothed)')\n",
    "plt.title('LMSç®—æ³•æ”¶æ•›è¿‡ç¨‹ï¼ˆå‡æ–¹è¯¯å·®ï¼‰')\n",
    "plt.xlabel('è¿­ä»£æ­¥ n')\n",
    "plt.ylabel('ç¬æ—¶è¯¯å·®å¹³æ–¹ $e^2(n)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
